{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML Course, Bogotá, Colombia  (&copy; Josh Bloom; June 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "\n",
       ".rendered_html\n",
       "{\n",
       "  color: #2C5494;\n",
       "  font-family: Ubuntu;\n",
       "  font-size: 140%;\n",
       "  line-height: 1.1;\n",
       "  margin: 0.5em 0;\n",
       "  }\n",
       "\n",
       ".talk_title\n",
       "{\n",
       "  color: #498AF3;\n",
       "  font-size: 250%;\n",
       "  font-weight:bold;\n",
       "  line-height: 1.2; \n",
       "  margin: 10px 50px 10px;\n",
       "  }\n",
       "\n",
       ".subtitle\n",
       "{\n",
       "  color: #386BBC;\n",
       "  font-size: 180%;\n",
       "  font-weight:bold;\n",
       "  line-height: 1.2; \n",
       "  margin: 20px 50px 20px;\n",
       "  }\n",
       "\n",
       ".slide-header, p.slide-header\n",
       "{\n",
       "  color: #498AF3;\n",
       "  font-size: 200%;\n",
       "  font-weight:bold;\n",
       "  margin: 0px 20px 10px;\n",
       "  page-break-before: always;\n",
       "  text-align: center;\n",
       "  }\n",
       "\n",
       ".rendered_html h1\n",
       "{\n",
       "  color: #498AF3;\n",
       "  line-height: 1.2; \n",
       "  margin: 0.15em 0em 0.5em;\n",
       "  page-break-before: always;\n",
       "  text-align: center;\n",
       "  }\n",
       "\n",
       "\n",
       ".rendered_html h2\n",
       "{ \n",
       "  color: #386BBC;\n",
       "  line-height: 1.2;\n",
       "  margin: 1.1em 0em 0.5em;\n",
       "  }\n",
       "\n",
       ".rendered_html h3\n",
       "{ \n",
       "  font-size: 100%;\n",
       "  line-height: 1.2;\n",
       "  margin: 1.1em 0em 0.5em;\n",
       "  }\n",
       "\n",
       ".rendered_html li\n",
       "{\n",
       "  line-height: 1.8;\n",
       "  }\n",
       "\n",
       ".input_prompt, .CodeMirror-lines, .output_area\n",
       "{\n",
       "  font-family: Consolas;\n",
       "  font-size: 120%;\n",
       "  }\n",
       "\n",
       ".gap-above\n",
       "{\n",
       "  padding-top: 200px;\n",
       "  }\n",
       "\n",
       ".gap01\n",
       "{\n",
       "  padding-top: 10px;\n",
       "  }\n",
       "\n",
       ".gap05\n",
       "{\n",
       "  padding-top: 50px;\n",
       "  }\n",
       "\n",
       ".gap1\n",
       "{\n",
       "  padding-top: 100px;\n",
       "  }\n",
       "\n",
       ".gap2\n",
       "{\n",
       "  padding-top: 200px;\n",
       "  }\n",
       "\n",
       ".gap3\n",
       "{\n",
       "  padding-top: 300px;\n",
       "  }\n",
       "\n",
       ".emph\n",
       "{\n",
       "  color: #386BBC;\n",
       "  }\n",
       "\n",
       ".warn\n",
       "{\n",
       "  color: red;\n",
       "  }\n",
       "\n",
       ".center\n",
       "{\n",
       "  text-align: center;\n",
       "  }\n",
       "\n",
       ".nb_link\n",
       "{\n",
       "    padding-bottom: 0.5em;\n",
       "}\n",
       "\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run ../talktools.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.asimovinstitute.org/wp-content/uploads/2016/09/neuralnetworks.png\">\n",
    "\n",
    "Source: http://www.asimovinstitute.org/neural-network-zoo/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.evernote.com/l/AUU9p3_1J5NJX61cCzZPOPc76jm68et-pUgB/image.png\">\n",
    "\n",
    "Source: http://www.wsdm-conference.org/2016/slides/WSDM2016-Jeff-Dean.pdf\n",
    "This is a good, high-level overview of what's important/current in DNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some references/Statements\n",
    "\n",
    "- \"Deep learning\" (Nature 2015) http://www.nature.com/nature/journal/v521/n7553/full/nature14539.html\n",
    "\n",
    "- \"HYPING ARTIFICIAL INTELLIGENCE, YET AGAIN\" http://www.newyorker.com/tech/elements/hyping-artificial-intelligence-yet-again\n",
    "\n",
    "- *\"Creating a deep learning model is, ironically, a highly manual process. Training a model takes a long time, and even for the top practitioners, it is a hit or miss affair where you don’t know whether it will work until the end. No mature tools exist to ensure models train successfully, or to ensure that the original set up is done appropriately for the data.\"* -- J. Howard (Fast.ai; http://www.fast.ai/2016/10/07/fastai-launch/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Nets (ConvNets)\n",
    "\n",
    "NNs built for images (or more generally, inputs with spatial structure).\n",
    "\n",
    "### Key Ideas: \n",
    "  - layers see only parts of each image (effectively all other weights are zero).\n",
    "  - some layers do simple operations on previous layers to reduce dimensionality (e.g., take the largest value in a a 3x3 range)\n",
    "  - \"Every Layer has a simple API: It transforms an input 3D volume to an output 3D volume with some differentiable function that may or may not have parameters.\"\n",
    " \n",
    "<img src=\"http://cs231n.github.io/assets/cnn/cnn.jpeg\">\n",
    "\n",
    "<img src=\"http://cs231n.github.io/assets/cnn/depthcol.jpeg\">\n",
    "\n",
    "\"An example input volume in red (e.g. a 32x32x3 CIFAR-10 image), and an example volume of neurons in the first Convolutional layer. Each neuron in the convolutional layer is connected only to a local region in the input volume spatially, but to the full depth (i.e. all color channels). Note, there are multiple neurons (5 in this example) along the depth, all looking at the same region in the input - see discussion of depth columns in text below. \"\n",
    "\n",
    "cf. http://cs231n.github.io/convolutional-networks/\n",
    "\n",
    "<img src=\"http://www.nature.com/nature/journal/v521/n7553/images/nature14539-f2.jpg\">\n",
    "Source: http://www.nature.com/nature/journal/v521/n7553/fig_tab/nature14539_F2.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter banks\n",
    "\n",
    "  http://setosa.io/ev/image-kernels/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling\n",
    "\n",
    "<img src=\"http://cs231n.github.io/assets/cnn/pool.jpeg\" width=\"40%\">\n",
    "<img src=\"http://cs231n.github.io/assets/cnn/maxpool.jpeg\" width=\"40%\">\n",
    "Source: http://cs231n.github.io/convolutional-networks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "import datetime, os\n",
    "\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Activation, BatchNormalization\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "# Print keras version\n",
    "print(tensorflow.keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28, 1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "nb_classes = 10\n",
    "batch_size = 128\n",
    "\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = fashion_mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # scale the images to 0-1\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train =  to_categorical(y_train, nb_classes)\n",
    "Y_test =  to_categorical(y_test, nb_classes)\n",
    "\n",
    "input_shape = x_train[0].shape  + (1,)\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 1.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add with tf.device('/gpu:0'): if on GPU\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(rate=0.1))\n",
    "\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(rate=0.1))\n",
    "\n",
    "\n",
    "model.add(Conv2D(128, (3,3), input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(rate=0.1))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "\n",
    "# Dropout\n",
    "model.add(Dropout(rate=0.5))\n",
    "    \n",
    "model.add(Dense(32, activation='relu'))\n",
    "    \n",
    "\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 26, 26, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 26, 26, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 11, 11, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 3, 3, 128)         73856     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               147584    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 244,842\n",
      "Trainable params: 244,778\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ... nn_results/colombia_nn_2019-06-06T04:00.h5\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.6719 - accuracy: 0.7578"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0605 23:01:43.559432 140735763825472 callbacks.py:1833] Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "W0605 23:01:43.560703 140735763825472 callbacks.py:1259] Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "W0605 23:01:43.561464 140735763825472 callbacks.py:989] Can save best model only with val_acc available, skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 59s 978us/sample - loss: 0.6715 - accuracy: 0.7579 - val_loss: 0.4494 - val_accuracy: 0.8439\n",
      "Epoch 2/20\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.4092 - accuracy: 0.8559"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0605 23:02:38.601661 140735763825472 callbacks.py:1833] Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "W0605 23:02:38.602554 140735763825472 callbacks.py:1259] Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "W0605 23:02:38.603157 140735763825472 callbacks.py:989] Can save best model only with val_acc available, skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 55s 917us/sample - loss: 0.4091 - accuracy: 0.8558 - val_loss: 0.3348 - val_accuracy: 0.8782\n",
      "Epoch 3/20\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.3473 - accuracy: 0.8770"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0605 23:03:34.455870 140735763825472 callbacks.py:1833] Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "W0605 23:03:34.456865 140735763825472 callbacks.py:1259] Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "W0605 23:03:34.457425 140735763825472 callbacks.py:989] Can save best model only with val_acc available, skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 56s 931us/sample - loss: 0.3475 - accuracy: 0.8769 - val_loss: 0.3719 - val_accuracy: 0.8572\n",
      "Epoch 4/20\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.3228 - accuracy: 0.8845"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0605 23:04:22.787962 140735763825472 callbacks.py:1833] Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "W0605 23:04:22.788997 140735763825472 callbacks.py:1259] Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "W0605 23:04:22.789598 140735763825472 callbacks.py:989] Can save best model only with val_acc available, skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 48s 806us/sample - loss: 0.3230 - accuracy: 0.8845 - val_loss: 0.2923 - val_accuracy: 0.8921\n",
      "Epoch 5/20\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.2956 - accuracy: 0.8935"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0605 23:05:14.777791 140735763825472 callbacks.py:1833] Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "W0605 23:05:14.778759 140735763825472 callbacks.py:1259] Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "W0605 23:05:14.779596 140735763825472 callbacks.py:989] Can save best model only with val_acc available, skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 52s 866us/sample - loss: 0.2954 - accuracy: 0.8935 - val_loss: 0.2874 - val_accuracy: 0.8955\n",
      "Epoch 6/20\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.2776 - accuracy: 0.8986"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0605 23:06:19.383963 140735763825472 callbacks.py:1833] Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "W0605 23:06:19.385648 140735763825472 callbacks.py:1259] Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "W0605 23:06:19.387066 140735763825472 callbacks.py:989] Can save best model only with val_acc available, skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 65s 1ms/sample - loss: 0.2777 - accuracy: 0.8986 - val_loss: 0.2845 - val_accuracy: 0.8969\n",
      "Epoch 7/20\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.2621 - accuracy: 0.9051"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0605 23:07:45.907226 140735763825472 callbacks.py:1833] Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "W0605 23:07:45.908896 140735763825472 callbacks.py:1259] Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "W0605 23:07:45.910547 140735763825472 callbacks.py:989] Can save best model only with val_acc available, skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 87s 1ms/sample - loss: 0.2621 - accuracy: 0.9052 - val_loss: 0.2888 - val_accuracy: 0.8938\n",
      "Epoch 8/20\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.2542 - accuracy: 0.9068"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0605 23:09:38.149214 140735763825472 callbacks.py:1833] Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "W0605 23:09:38.151090 140735763825472 callbacks.py:1259] Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "W0605 23:09:38.152343 140735763825472 callbacks.py:989] Can save best model only with val_acc available, skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 112s 2ms/sample - loss: 0.2543 - accuracy: 0.9067 - val_loss: 0.2629 - val_accuracy: 0.8996\n",
      "Epoch 9/20\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.2410 - accuracy: 0.9134"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0605 23:11:44.073169 140735763825472 callbacks.py:1833] Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "W0605 23:11:44.077711 140735763825472 callbacks.py:1259] Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "W0605 23:11:44.084103 140735763825472 callbacks.py:989] Can save best model only with val_acc available, skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 126s 2ms/sample - loss: 0.2413 - accuracy: 0.9133 - val_loss: 0.2616 - val_accuracy: 0.9077\n",
      "Epoch 10/20\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.2352 - accuracy: 0.9134"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0605 23:12:53.116400 140735763825472 callbacks.py:1833] Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "W0605 23:12:53.117266 140735763825472 callbacks.py:1259] Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "W0605 23:12:53.117769 140735763825472 callbacks.py:989] Can save best model only with val_acc available, skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 69s 1ms/sample - loss: 0.2353 - accuracy: 0.9134 - val_loss: 0.2761 - val_accuracy: 0.9007\n",
      "Epoch 11/20\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.2263 - accuracy: 0.9164"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0605 23:13:49.085966 140735763825472 callbacks.py:1833] Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "W0605 23:13:49.087198 140735763825472 callbacks.py:1259] Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "W0605 23:13:49.087944 140735763825472 callbacks.py:989] Can save best model only with val_acc available, skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 56s 933us/sample - loss: 0.2262 - accuracy: 0.9165 - val_loss: 0.2512 - val_accuracy: 0.9094\n",
      "Epoch 12/20\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.2162 - accuracy: 0.9204"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0605 23:14:40.500149 140735763825472 callbacks.py:1833] Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "W0605 23:14:40.501179 140735763825472 callbacks.py:1259] Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "W0605 23:14:40.501929 140735763825472 callbacks.py:989] Can save best model only with val_acc available, skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 51s 857us/sample - loss: 0.2162 - accuracy: 0.9203 - val_loss: 0.2558 - val_accuracy: 0.9092\n",
      "Epoch 13/20\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.2108 - accuracy: 0.9221"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0605 23:15:39.190994 140735763825472 callbacks.py:1833] Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "W0605 23:15:39.192843 140735763825472 callbacks.py:1259] Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "W0605 23:15:39.193822 140735763825472 callbacks.py:989] Can save best model only with val_acc available, skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 59s 978us/sample - loss: 0.2108 - accuracy: 0.9222 - val_loss: 0.2498 - val_accuracy: 0.9104\n",
      "Epoch 14/20\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.2022 - accuracy: 0.9254"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0605 23:16:34.172033 140735763825472 callbacks.py:1833] Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "W0605 23:16:34.173079 140735763825472 callbacks.py:1259] Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "W0605 23:16:34.173583 140735763825472 callbacks.py:989] Can save best model only with val_acc available, skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 55s 916us/sample - loss: 0.2022 - accuracy: 0.9254 - val_loss: 0.2558 - val_accuracy: 0.9061\n",
      "Epoch 15/20\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.1974 - accuracy: 0.9275"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0605 23:17:36.789813 140735763825472 callbacks.py:1833] Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "W0605 23:17:36.791057 140735763825472 callbacks.py:1259] Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "W0605 23:17:36.791671 140735763825472 callbacks.py:989] Can save best model only with val_acc available, skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 63s 1ms/sample - loss: 0.1975 - accuracy: 0.9275 - val_loss: 0.3110 - val_accuracy: 0.8880\n",
      "Epoch 16/20\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.1926 - accuracy: 0.9298"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0605 23:18:28.782324 140735763825472 callbacks.py:1833] Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "W0605 23:18:28.783318 140735763825472 callbacks.py:1259] Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "W0605 23:18:28.783909 140735763825472 callbacks.py:989] Can save best model only with val_acc available, skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 52s 867us/sample - loss: 0.1925 - accuracy: 0.9298 - val_loss: 0.2514 - val_accuracy: 0.9124\n",
      "Epoch 17/20\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.1871 - accuracy: 0.9306"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0605 23:19:19.384387 140735763825472 callbacks.py:1833] Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "W0605 23:19:19.385722 140735763825472 callbacks.py:1259] Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "W0605 23:19:19.386244 140735763825472 callbacks.py:989] Can save best model only with val_acc available, skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 51s 843us/sample - loss: 0.1871 - accuracy: 0.9305 - val_loss: 0.2505 - val_accuracy: 0.9140\n",
      "Epoch 18/20\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.1847 - accuracy: 0.9310"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0605 23:20:13.004616 140735763825472 callbacks.py:1833] Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "W0605 23:20:13.005621 140735763825472 callbacks.py:1259] Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "W0605 23:20:13.006238 140735763825472 callbacks.py:989] Can save best model only with val_acc available, skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 54s 894us/sample - loss: 0.1847 - accuracy: 0.9311 - val_loss: 0.2627 - val_accuracy: 0.9106\n",
      "Epoch 19/20\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.1741 - accuracy: 0.9356"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0605 23:21:04.693963 140735763825472 callbacks.py:1833] Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "W0605 23:21:04.695279 140735763825472 callbacks.py:1259] Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "W0605 23:21:04.695908 140735763825472 callbacks.py:989] Can save best model only with val_acc available, skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 52s 861us/sample - loss: 0.1740 - accuracy: 0.9356 - val_loss: 0.2890 - val_accuracy: 0.9022\n",
      "Epoch 20/20\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.1744 - accuracy: 0.9345"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0605 23:21:56.570001 140735763825472 callbacks.py:1833] Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "W0605 23:21:56.571504 140735763825472 callbacks.py:1259] Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "W0605 23:21:56.573377 140735763825472 callbacks.py:989] Can save best model only with val_acc available, skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "60000/60000 [==============================] - 52s 865us/sample - loss: 0.1743 - accuracy: 0.9346 - val_loss: 0.2562 - val_accuracy: 0.9143\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xb36922e10>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "run_time_string = datetime.datetime.utcnow().isoformat(timespec='minutes')\n",
    "# define path to save model\n",
    "model_path = f'nn_results/colombia_nn_{run_time_string}.h5'\n",
    "print(f\"Training ... {model_path}\")\n",
    "\n",
    "logdir = os.path.join(\"nn_results\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.75,\n",
    "                              patience=2, min_lr=1e-6, verbose=1, cooldown=0)\n",
    "\n",
    "csv_logger = tf.keras.callbacks.CSVLogger(f'nn_results/training_{run_time_string}.log')\n",
    "\n",
    "earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0.001, \n",
    "                                             patience=3, \\\n",
    "                                             verbose=1, mode='auto')\n",
    "\n",
    "model_check = tf.keras.callbacks.ModelCheckpoint(model_path,\n",
    "        monitor='val_accuracy', \n",
    "        save_best_only=True, \n",
    "        mode='max',\n",
    "        verbose=1)\n",
    "\n",
    "model.fit(x=x_train.reshape(-1,28,28,1), \n",
    "          y=Y_train, \n",
    "          epochs=20,\n",
    "          batch_size=batch_size,\n",
    "          validation_data=(x_test.reshape(-1,28,28,1), Y_test), \n",
    "          callbacks=[tensorboard_callback, reduce_lr, csv_logger, earlystop, model_check])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of the layers\n",
    "\n",
    "From François Chollet (“DEEP LEARNING with Python”):\n",
    "\n",
    "Intermediate activations are “useful for understanding how successive convnet layers transform their input, and for getting a first idea of the meaning of individual convnet filters.”\n",
    "\n",
    "“The representations learned by convnets are highly amenable to visualization, in large part because they’re representations of visual concepts. Visualizing intermediate activations consists of displaying the feature maps that are output by various convolution and pooling layers in a network, given a certain input (the output of a layer is often called its activation, the output of the activation function). This gives a view into how an input is decomposed into the different filters learned by the network. Each channel encodes relatively independent features, so the proper way to visualize these feature maps is by independently plotting the contents of every channel as a 2D image.”\n",
    "\n",
    "Following from https://github.com/gabrielpierobon/cnnshapes/blob/master/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_outputs = []\n",
    "layer_names = []\n",
    "for layer in model.layers:\n",
    "    keep = True\n",
    "    for t in [\"dropout\", \"normalization\", \"flatten\", \"dense\", \"activation\"]:\n",
    "        if layer.name.find(t) != -1:\n",
    "            keep = False\n",
    "    if keep:\n",
    "        layer_outputs.append(layer.output)\n",
    "        layer_names.append(layer.name) \n",
    "        \n",
    "activation_model = tensorflow.keras.models.Model(inputs=model.input, outputs=layer_outputs)\n",
    "activations = activation_model.predict(x_test[2].reshape(1,28,28,1)) # Returns a list of five Numpy arrays: one array per layer activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_layer_activation = activations[0]\n",
    "print(first_layer_activation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "images_per_row = 15\n",
    "\n",
    "for layer_name, layer_activation in zip(layer_names, activations): # Displays the feature maps\n",
    "    n_features = layer_activation.shape[-1]   # Number of features in the feature map\n",
    "    size = layer_activation.shape[1] #The feature map has shape (1, size, size, n_features).\n",
    "    n_cols = n_features // images_per_row # Tiles the activation channels in this matrix\n",
    "    display_grid = np.zeros((size * n_cols, images_per_row * size))\n",
    "    for col in range(n_cols): # Tiles each filter into a big horizontal grid\n",
    "        for row in range(images_per_row):\n",
    "            channel_image = layer_activation[0,\n",
    "                                             :, :,\n",
    "                                             col * images_per_row + row]\n",
    "            channel_image -= channel_image.mean() # Post-processes the feature to make it visually palatable\n",
    "            channel_image /= channel_image.std()\n",
    "            channel_image *= 64\n",
    "            channel_image += 128\n",
    "            channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n",
    "            display_grid[col * size : (col + 1) * size, # Displays the grid\n",
    "                         row * size : (row + 1) * size] = channel_image\n",
    "    scale = 1. / size\n",
    "    plt.figure(figsize=(scale * display_grid.shape[1],\n",
    "                        scale * display_grid.shape[0]))\n",
    "    plt.title(layer_name)\n",
    "    plt.grid(False)\n",
    "    plt.imshow(display_grid, aspect='auto', cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1200/1*C8hNiOqur4OJyEZmC7OnzQ.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "\n",
    "# creating the data aumentation genreators for both the training images and the training label masks\n",
    "data_gen_args = dict(featurewise_center=False,\n",
    "                     featurewise_std_normalization=False,\n",
    "                     rotation_range=15.,\n",
    "                     rescale=1.0,\n",
    "                     shear_range=0.1,\n",
    "                     zoom_range=0.1,\n",
    "                     horizontal_flip=True,\n",
    "                     fill_mode = 'nearest')\n",
    "\n",
    "image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "\n",
    "seed = 42\n",
    "batch_size = 128\n",
    "\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = fashion_mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # scale the images to 0-1\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train =  to_categorical(y_train, nb_classes)\n",
    "Y_test =  to_categorical(y_test, nb_classes)\n",
    "\n",
    "input_shape = x_train[0].shape  + (1,)\n",
    "\n",
    "image_generator = image_datagen.flow(\n",
    "    np.expand_dims(x_train, axis=3),  Y_train,\n",
    "    shuffle = True,\n",
    "    batch_size=batch_size,\n",
    "    seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keras_preprocessing.image.NumpyArrayIterator"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(image_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rez = image_generator.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 28, 28, 1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rez.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0xb53d32898>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAFSpJREFUeJzt3X1w1dWZB/Dvc3PvTUIIBBACCkJQFqG+UJoFLbZldbXiuous1sp0le50xVnrrI7d7TrMdmQ609HZ2frS3dVdVEbs+Dpbreg4bR22O+pKkcCqiLHgC0IgJEBAQt5v7rN/5NKJmvOc63373Xi+nxkmyX3uub/DTb753ZvzO+eIqoKIwhOLugNEFA2GnyhQDD9RoBh+okAx/ESBYviJAsXwEwWK4ScKFMNPFKh4KQ+WlEqtQk0pD5k1rR1j1xPirMX60vaDd/Xk0iWiz60XXejXPvcP6zB5hV9ELgNwH4AKAA+p6l3W/atQg8VycT6HzJ3Yz0dq8UKz3jM54ayN3WuHW15706wjVmHX04N2vZg8zxt4eXhZ2aKbsr5vzi/7RaQCwL8DWAZgPoCVIjI/18cjotLK5z3/IgDvqeoHqtoP4EkAywvTLSIqtnzCfxqAfcO+bsnc9gkislpEmkSkaQB9eRyOiAopn/CP9GbwM28AVXWdqjaqamMClXkcjogKKZ/wtwCYMezr6QAO5NcdIiqVfMK/FcAcEWkQkSSAawFsLEy3iKjYch7qU9WUiNwM4NcYGupbr6o7C9azQvMMSVVue8+sDyyd66x1zqo2245LnWPW8foOuz6aWUOF4jn3RDnEGYC8xvlV9UUALxaoL0RUQry8lyhQDD9RoBh+okAx/ESBYviJAsXwEwWqpPP5y9ngsY/Neu0298WLLVefbrbt+NJYs35m2wyznvpon1nPi2c6sVTYdU0N2I9vXF9R8UcN9rH7+u3HHkiZ5dR+XnBq4ZmfKFAMP1GgGH6iQDH8RIFi+IkCxfATBYpDfVlKdxx1F79Ra7fttlcwal023axPfeqEWR88avQtT76hPFlor9l65Lxxzlqqyl4ZOJ00y+idZE/TPuNu96rKxXzORgue+YkCxfATBYrhJwoUw08UKIafKFAMP1GgGH6iQHGcP0u7157rrE2ubDfbTqzpNuv7l9pbfLfG55n1Kf/2mlk3qX3snuV/bNY7T7V/hAaMSyCm391kto3N/Mzub5/QPfcUs97y1+7nbdrdeTxnXxA88xMFiuEnChTDTxQohp8oUAw/UaAYfqJAMfxEgcprnF9E9gDoBDAIIKWqjYXoVBQ6rz3frDcsbHHWOrrtLboTMXssfcXct8z6M+kFZn1i81ecNd/W4+kGey2Bnr85ZtYn3znGrCc+OuSsDcbs+fyD731o1is99YqGrzprrbe5a0AY1wEU4iKfP1HVwwV4HCIqIb7sJwpUvuFXAL8RkW0isroQHSKi0sj3Zf8SVT0gIlMAvCQi76rqy8PvkPmlsBoAqmC/PySi0snrzK+qBzIf2wE8C2DRCPdZp6qNqtqYgL2QJRGVTs7hF5EaEak9+TmASwG8XaiOEVFx5fOyvx7AsyJy8nEeV9VfFaRXRFR0OYdfVT8AcF4B+1JU8dmzzPrAX3WY9cMnapw133z9+upOs958fKpZ//b8bWb9+XO/5qxVT7PXAmj72qBZP/0/7bH42JbtZj2Vcm+jHauqMtvaq/L7nfKG+/tyeAH//sShPqJAMfxEgWL4iQLF8BMFiuEnChTDTxSoYJbu/nDlqWY9nrK3bK6vdW+TPbnK3kK7LuneKhoAmg/Xm/XjffaQ2MTL9ztr/YMVZtvkNvvYVc9vNuve4ThxDxWm++3tv/OlCfe5LfZNz0TU++3nzbfkOTTfgcri45mfKFAMP1GgGH6iQDH8RIFi+IkCxfATBYrhJwrUF2acP32hvbx1T0O/WZ89tsuu17rHhesS9jj+64dnmvXaqj6zfqjTPZ0YAOpq3MdvGGdPVU6+Msmsx8bYU1/TvXbfzfHwtD2d2LpGYOix7bH0RIf7eencaf+/p9S02ofusb/nakxlLhc88xMFiuEnChTDTxQohp8oUAw/UaAYfqJAMfxEgRpV4/wVdeOdtSNn2dtkXzDvXbM+pcpeXtvS3ldr1lXt8equ/oRZ72kda9ZrjGsYYmLPO99ztT1WPrvfXvo7/t/2suLesXpLnnPi2xfXuYue6fjpTs/PQ8wz338U4JmfKFAMP1GgGH6iQDH8RIFi+IkCxfATBYrhJwqUd5xfRNYDuAJAu6qenbltIoCnAMwCsAfANapqL3xfADJ+nLPWs+y42XZa1cdmPSb2mPLxlHvt/EO99jj8uMpesz42ac+Jr5ljr28/kHb/Dm/af7rZdvG8D8x607K5Zn12j71Lu2x+0130jZV75vv71nDoWOx+3ubetMNsq961BDwXCowC2Zz5HwFw2aduux3AJlWdA2BT5msiGkW84VfVlwF8ejmY5QA2ZD7fAODKAveLiIos1/f89araCgCZj1MK1yUiKoWiX9svIqsBrAaAKtjrwRFR6eR65m8TkWkAkPnY7rqjqq5T1UZVbUygMsfDEVGh5Rr+jQBWZT5fBeC5wnSHiErFG34ReQLAZgBzRaRFRL4H4C4Al4jIbgCXZL4molHE+55fVVc6ShcXuC9e+66a4axdPHOr2dY3jt+Xtp+KI33utfPHJuxx+rRnPr+v7nu3dKTH/bcU8fy/3zlUb9a/c+nLZv2/Or5h1mcem+OsDTbvNtvGqtzXVgDAruvsdRAmveb+nopnHF/zXEtgNOAVfkSBYviJAsXwEwWK4ScKFMNPFCiGnyhQZbV0d2zBfLN+6hUfOWtTk/aU3ve7J5t131CfNRznG6pLGVNuh9rbdWsoDwBSg+6psb6Zqb299nDZr/fbS3efd0WzWd998CxnbXL7EbPtiQvPNOu19fby2sku91Lv6V57mnVeS44DeS87Xgo88xMFiuEnChTDTxQohp8oUAw/UaAYfqJAMfxEgSqrcf4PVxhbKgO4dJx7CuiBPrtt12DSrMdgj8taU4J94/gpzW8756p4yqz3GNcZ9PfZ3+LBlN33w8fsZcnjMXsJ685lJ5y12n0NZtsj8+2+L5+506z/3/PurdPTvmXDfUtzj4JxfB+e+YkCxfATBYrhJwoUw08UKIafKFAMP1GgGH6iQJV2nL+mGrrAvaVzzcLDZnNrzv5rHbPNtlUV9jbXVRX2WLo1zp/30twevrF0q56stP9f3X328tixCnub7APt9vUV06Ycc9b2/IW9TsHtl/zSrD9853KzXte12V30bsE9+sfxfXjmJwoUw08UKIafKFAMP1GgGH6iQDH8RIFi+IkC5R3nF5H1AK4A0K6qZ2duWwvgBgCHMndbo6oveo8WE6ST7nnUT5+73mz+o5Y/d9bGJex12OMxe7zaJ26NC3t+haY8Y8b5XgdgXYMwfkyP2Tblmc+vnr5VVfeb9UNH3XPq62a5rwEAgIc+WGLWx+21t0Y3ieebpvn9vIwG2Zz5HwFw2Qi336OqCzL//MEnorLiDb+qvgygowR9IaISyuc9/80i8paIrBeRCQXrERGVRK7hfwDAGQAWAGgF8FPXHUVktYg0iUhTf39XjocjokLLKfyq2qaqg6qaBvAggEXGfdepaqOqNiaTNbn2k4gKLKfwi8i0YV+uAPB2YbpDRKWSzVDfEwCWAjhFRFoA3AFgqYgsAKAA9gC4sYh9JKIi8IZfVVeOcPPDuRwsHRd017vXz3+m0z3XHwDqku4x64M97vFkAIir/SInmcd1ANY4OwDExX7stOcFWEo8a8gbKjx9m1DbbdZP9Faa9YRnvr9WGnsKpOwfv+njj5j19y+YbNZndJ3j7tfWHWbbEPAKP6JAMfxEgWL4iQLF8BMFiuEnChTDTxSoki7dHTvahdqnfuesr1v+NbP9X857w1k7HreXoD4xYA9Z+YbrrC28fdt7Jz3DYam03X6MZ6jPN+3WEhu0fwRSCXvp7/6UvdV1Ou3uW02VPR24zzMUeNnV7p8lAPiVnO+szfz4DLPt4K73zbrE7b5pyn7eygHP/ESBYviJAsXwEwWK4ScKFMNPFCiGnyhQDD9RoEq7RbfHmTfaY6uvPzPTWWuctNdsu0/tZQZ7UwmzPibuHpNOwx5n910H4LvGwKc67t5+POWZyuzre3XC3tr8tNqPzXrMuEahtWuc2fZwt72F9/txe0rvhK8fdNYOHq0320496t4OHgAGj3jWtB0FW4DzzE8UKIafKFAMP1GgGH6iQDH8RIFi+IkCxfATBaqsxvnTnZ1mPfmjBmdtyc93m23/R88y63u7PNcBGPPeffP1+9P2nHfvWgKeum+s3lLhWStgbNLeBtu39Xl/2v28+ZYVr4jZ9U7PGg3zJrQ5a1u+abc9iDPN+uQHNpv10YBnfqJAMfxEgWL4iQLF8BMFiuEnChTDTxQohp8oUKKeecUiMgPAowCmAkgDWKeq94nIRABPAZgFYA+Aa1T1qPVY42SiLpaLC9DtzzpywwVm/aE195r1f22z+3W0v/pz9+mkfMfx456x+O6Ue9vzfK4ByEb/oH0Nw4BxjcNg2j73VBnrFABAXaV7y3YASBmPv2B8i9n2kbfda/4DwNwfHrKP3bLfrBfLFt2E49qR1Tc9mzN/CsAPVHUegPMBfF9E5gO4HcAmVZ0DYFPmayIaJbzhV9VWVd2e+bwTQDOA0wAsB7Ahc7cNAK4sVieJqPA+13t+EZkF4MsAtgCoV9VWYOgXBIAphe4cERVP1uEXkbEAfgHgVlW1Fzj7ZLvVItIkIk0DsK8TJ6LSySr8IpLAUPAfU9VnMje3ici0TH0agPaR2qrqOlVtVNXGBOzJFERUOt7wi4gAeBhAs6rePay0EcCqzOerADxX+O4RUbFkM6V3CYDrAOwQkZN7ZK8BcBeAp0XkewD2AvhWcbqYnUkP2lMsr/rSLWb96eU/M+v3t13krB3pqzHbJivsISvf0t6+4TrftFpLyjPd2H9sexjSGuoT3xCn57GtIU4AuLq+yVm745UVZtvqj+yl3FMH3MuCjxbe8Kvqq4DzJ6A4g/ZEVHS8wo8oUAw/UaAYfqJAMfxEgWL4iQLF8BMFqqyW7i6mM2/9nVn/u7OuNes/nuO+hunxQ/Z04rbeWrNel7SnpvquA7B+hac1vym9vmP7tgA/3lvlrCXjKbNtbdI+tjWODwA/efNyZ236i3a/a16wH1vTuV9bUS545icKFMNPFCiGnyhQDD9RoBh+okAx/ESBYviJAhXMOL/PuKvc2zkDwMHtdc7aqimvmm1/2znfrB/oG2/WE56luw/32+sJ5MM3n7/txFizrsZ1BpOqu822SyftMutpz7mr4TvvOGsnXjjdbCu7Z5t13fl7u32FvU6CpuxrHEqBZ36iQDH8RIFi+IkCxfATBYrhJwoUw08UKIafKFAc589Id3WZ9fV/696HdNX99n4l4+P2ePb0ZIdZf72zwawnY+4x4/60/S32rfl/tG+MWU97ttmuG+Neq+D8CR+aba8fv8Osr7jlNrPevtbdt8fn3me2/Sdcb9bh2dpe0741GIzrAEq0VgDP/ESBYviJAsXwEwWK4ScKFMNPFCiGnyhQDD9RoLzj/CIyA8CjAKYCSANYp6r3ichaADcAOJS56xpVfbFYHY1acsu7ztqdT15jto312Y/96k3/YtY70+617wFgR+d0Z60ybh/8iGccv6s/adZ7++197JfM3Omsza/eb7b96mN/b9Znb9xq1gcu+oqztj/lXp8BAGKHj5p1e4UFeMfqJe6OnsJeC6BQ1wFkc5FPCsAPVHW7iNQC2CYiL2Vq96iq/ZNLRGXJG35VbQXQmvm8U0SaAZxW7I4RUXF9rvf8IjILwJcBbMncdLOIvCUi60VkgqPNahFpEpGmAXhe/xJRyWQdfhEZC+AXAG5V1eMAHgBwBoAFGHpl8NOR2qnqOlVtVNXGBCoL0GUiKoSswi8iCQwF/zFVfQYAVLVNVQdVNQ3gQQCLitdNIio0b/hFRAA8DKBZVe8edvu0YXdbAeDtwnePiIolm7/2LwFwHYAdIvJG5rY1AFaKyAIACmAPgBuL0sMyYU357Zvda7atqbXri165yazvXvqIWb+pe6pZt/im/B47UW3WG6fvM+vTksectR83/5nZtuH2zWZdrWmxAH540QvO2h33ftdsO+Xga2Y9X9bS3ZKwh1eHXmy7itn3IZu/9r8KjLh4+xd2TJ8oBLzCjyhQDD9RoBh+okAx/ESBYviJAsXwEwWKS3cXwFm3fWTWB5+yx8p3tZ5q1hc2fdusP3jOz521e1ovNdu2dtaa9flTD5r166f8r1n/WcufOmtT/8GemuqbuLrrPxaa9Q/73FN+J2+3l2qPkqYGzLrEjWnUA/aW6sPxzE8UKIafKFAMP1GgGH6iQDH8RIFi+IkCxfATBUrUs9VwQQ8mcgjA8EHxUwAcLlkHPp9y7Vu59gtg33JVyL7NVNXJ2dyxpOH/zMFFmlS1MbIOGMq1b+XaL4B9y1VUfePLfqJAMfxEgYo6/OsiPr6lXPtWrv0C2LdcRdK3SN/zE1F0oj7zE1FEIgm/iFwmIr8XkfdE5PYo+uAiIntEZIeIvCEiTRH3Zb2ItIvI28NumygiL4nI7szHEbdJi6hva0Vkf+a5e0NELo+obzNE5Lci0iwiO0XklsztkT53Rr8ied5K/rJfRCoA7AJwCYAWAFsBrFTVd0raEQcR2QOgUVUjHxMWka8DOAHgUVU9O3PbPwPoUNW7Mr84J6jqP5ZJ39YCOBH1zs2ZDWWmDd9ZGsCVAL6LCJ87o1/XIILnLYoz/yIA76nqB6raD+BJAMsj6EfZU9WXAXR86ublADZkPt+AoR+eknP0rSyoaquqbs983gng5M7SkT53Rr8iEUX4TwMwfJuXFpTXlt8K4Dcisk1EVkfdmRHUZ7ZNP7l9+pSI+/Np3p2bS+lTO0uXzXOXy47XhRZF+EdaZ6ichhyWqOpCAMsAfD/z8payk9XOzaUyws7SZSHXHa8LLYrwtwCYMezr6QAORNCPEanqgczHdgDPovx2H247uUlq5mN7xP35g3LauXmknaVRBs9dOe14HUX4twKYIyINIpIEcC2AjRH04zNEpCbzhxiISA2AS1F+uw9vBLAq8/kqAM9F2JdPKJedm107SyPi567cdryO5CKfzFDGvQAqAKxX1Z+UvBMjEJHZGDrbA0MrGz8eZd9E5AkASzE066sNwB0AfgngaQCnA9gL4FuqWvI/vDn6thRDL13/sHPzyffYJe7bhQBeAbADwMktbddg6P11ZM+d0a+ViOB54xV+RIHiFX5EgWL4iQLF8BMFiuEnChTDTxQohp8oUAw/UaAYfqJA/T8FRy4ZO0vsewAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(rez[0][:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add with tf.device('/gpu:0'): if on GPU\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(rate=0.1))\n",
    "\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(rate=0.1))\n",
    "\n",
    "\n",
    "model.add(Conv2D(128, (3,3), input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(rate=0.1))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "\n",
    "# Dropout\n",
    "model.add(Dropout(rate=0.5))\n",
    "    \n",
    "model.add(Dense(32, activation='relu'))\n",
    "    \n",
    "\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ... nn_results/colombia_nn_2019-06-06T13:48.h5\n",
      "Epoch 1/20\n",
      "468/469 [============================>.] - ETA: 0s - loss: 0.5279 - accuracy: 0.8074\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.84440, saving model to nn_results/colombia_nn_2019-06-06T13:48.h5\n",
      "469/469 [==============================] - 59s 126ms/step - loss: 0.5280 - accuracy: 0.8073 - val_loss: 0.4137 - val_accuracy: 0.8444\n",
      "Epoch 2/20\n",
      "362/469 [======================>.......] - ETA: 14s - loss: 0.4609 - accuracy: 0.8334"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-ad12373bd887>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m           callbacks=[tensorboard_callback, reduce_lr, csv_logger, earlystop, model_check])\n\u001b[0m",
      "\u001b[0;32m/Users/jbloom/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1431\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m/Users/jbloom/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jbloom/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1173\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_sample_weight_modes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1175\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jbloom/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3441\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3442\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3443\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3444\u001b[0m     return nest.pack_sequence_as(\n\u001b[1;32m   3445\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_outputs_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jbloom/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m    560\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m--> 561\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jbloom/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    658\u001b[0m     \u001b[0;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 660\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    661\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_register_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jbloom/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args)\u001b[0m\n\u001b[1;32m    432\u001b[0m             attrs=(\"executor_type\", executor_type,\n\u001b[1;32m    433\u001b[0m                    \"config_proto\", config),\n\u001b[0;32m--> 434\u001b[0;31m             ctx=ctx)\n\u001b[0m\u001b[1;32m    435\u001b[0m       \u001b[0;31m# Replace empty list with None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jbloom/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "run_time_string = datetime.datetime.utcnow().isoformat(timespec='minutes')\n",
    "# define path to save model\n",
    "model_path = f'nn_results/colombia_nn_{run_time_string}.h5'\n",
    "print(f\"Training ... {model_path}\")\n",
    "\n",
    "logdir = os.path.join(\"nn_results\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.75,\n",
    "                              patience=2, min_lr=1e-6, verbose=1, cooldown=0)\n",
    "\n",
    "csv_logger = tf.keras.callbacks.CSVLogger(f'nn_results/training_{run_time_string}.log')\n",
    "\n",
    "earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0.001, \n",
    "                                             patience=3, \\\n",
    "                                             verbose=1, mode='auto')\n",
    "\n",
    "model_check = tf.keras.callbacks.ModelCheckpoint(model_path,\n",
    "        monitor='val_accuracy', \n",
    "        save_best_only=True, \n",
    "        mode='max',\n",
    "        verbose=1)\n",
    "\n",
    "model.fit_generator(image_generator,\n",
    "          epochs=20,\n",
    "          validation_data=(x_test.reshape(-1,28,28,1), Y_test), \n",
    "          callbacks=[tensorboard_callback, reduce_lr, csv_logger, earlystop, model_check])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
