{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML Course, Bogotá, Colombia  (&copy; Josh Bloom; June 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "\n",
       ".rendered_html\n",
       "{\n",
       "  color: #2C5494;\n",
       "  font-family: Ubuntu;\n",
       "  font-size: 140%;\n",
       "  line-height: 1.1;\n",
       "  margin: 0.5em 0;\n",
       "  }\n",
       "\n",
       ".talk_title\n",
       "{\n",
       "  color: #498AF3;\n",
       "  font-size: 250%;\n",
       "  font-weight:bold;\n",
       "  line-height: 1.2; \n",
       "  margin: 10px 50px 10px;\n",
       "  }\n",
       "\n",
       ".subtitle\n",
       "{\n",
       "  color: #386BBC;\n",
       "  font-size: 180%;\n",
       "  font-weight:bold;\n",
       "  line-height: 1.2; \n",
       "  margin: 20px 50px 20px;\n",
       "  }\n",
       "\n",
       ".slide-header, p.slide-header\n",
       "{\n",
       "  color: #498AF3;\n",
       "  font-size: 200%;\n",
       "  font-weight:bold;\n",
       "  margin: 0px 20px 10px;\n",
       "  page-break-before: always;\n",
       "  text-align: center;\n",
       "  }\n",
       "\n",
       ".rendered_html h1\n",
       "{\n",
       "  color: #498AF3;\n",
       "  line-height: 1.2; \n",
       "  margin: 0.15em 0em 0.5em;\n",
       "  page-break-before: always;\n",
       "  text-align: center;\n",
       "  }\n",
       "\n",
       "\n",
       ".rendered_html h2\n",
       "{ \n",
       "  color: #386BBC;\n",
       "  line-height: 1.2;\n",
       "  margin: 1.1em 0em 0.5em;\n",
       "  }\n",
       "\n",
       ".rendered_html h3\n",
       "{ \n",
       "  font-size: 100%;\n",
       "  line-height: 1.2;\n",
       "  margin: 1.1em 0em 0.5em;\n",
       "  }\n",
       "\n",
       ".rendered_html li\n",
       "{\n",
       "  line-height: 1.8;\n",
       "  }\n",
       "\n",
       ".input_prompt, .CodeMirror-lines, .output_area\n",
       "{\n",
       "  font-family: Consolas;\n",
       "  font-size: 120%;\n",
       "  }\n",
       "\n",
       ".gap-above\n",
       "{\n",
       "  padding-top: 200px;\n",
       "  }\n",
       "\n",
       ".gap01\n",
       "{\n",
       "  padding-top: 10px;\n",
       "  }\n",
       "\n",
       ".gap05\n",
       "{\n",
       "  padding-top: 50px;\n",
       "  }\n",
       "\n",
       ".gap1\n",
       "{\n",
       "  padding-top: 100px;\n",
       "  }\n",
       "\n",
       ".gap2\n",
       "{\n",
       "  padding-top: 200px;\n",
       "  }\n",
       "\n",
       ".gap3\n",
       "{\n",
       "  padding-top: 300px;\n",
       "  }\n",
       "\n",
       ".emph\n",
       "{\n",
       "  color: #386BBC;\n",
       "  }\n",
       "\n",
       ".warn\n",
       "{\n",
       "  color: red;\n",
       "  }\n",
       "\n",
       ".center\n",
       "{\n",
       "  text-align: center;\n",
       "  }\n",
       "\n",
       ".nb_link\n",
       "{\n",
       "    padding-bottom: 0.5em;\n",
       "}\n",
       "\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run ../talktools.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.evernote.com/l/AUVOiUntlxZHM60VXN8WZWzg83pzEIL-XJwB/image.png\" width=\"50%\">\n",
    "\n",
    "Building Trees Rigorously (Node Splitting Criteria)\n",
    "\n",
    "<img src=\"https://www.evernote.com/l/AUVA6K4mqnhOMoNcy93La3lFe5XOAxgaWrUB/image.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collections of Trees (\"Decision Forests\", \"Random Forests\")\n",
    "\n",
    "<img src=\"https://contentmamluswest001.blob.core.windows.net/content/14b2744cf8d6418c87ffddc3f3127242/9502630827244d60a1214f250e3bbca7/b729c21014a34955b20fa94dc13390e5/image\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> \"**Bagging** is a simple ensembling technique in which we build many independent predictors/models/learners and combine them using some model averaging techniques. (e.g. weighted average, majority vote or normal average)\"\n",
    "\n",
    "https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying NIST Handwritten Digits\n",
    "\n",
    "We will try to classify handwritten digits (0-9) from their raw pixelated images.\n",
    "\n",
    "Each image is 8 $\\times$ 8 pixels.  We will not do any feature extraction and instead classify based on the intensity values for each pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets, metrics\n",
    "# import NIST digits data set (1797 8x8 images)\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "f,axs = plt.subplots(1,10,figsize=(16, 6))\n",
    "objarr = np.empty_like(axs)\n",
    "for n, ax in enumerate(axs.flat):\n",
    "    objarr.flat[n] = ax.imshow(digits['images'][n], cmap='gray', interpolation='nearest')\n",
    "    ax.get_xaxis().set_ticks([])\n",
    "    ax.get_yaxis().set_ticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Now we split the data into a training and testing set.}$\n",
    "\n",
    "$\\textbf{We will only fit the classifier on the training set and use the testing set to evaluate performance.}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the first 500 as training\n",
    "train = 500\n",
    "Xtr = digits['data'][:train]\n",
    "Ytr = digits['target'][:train]\n",
    "print(\"training size: \" + str(len(Ytr)))\n",
    "\n",
    "# testing set\n",
    "Xte = digits['data'][train:]\n",
    "Yte = digits['target'][train:]\n",
    "print(\"testing size: \" + str(len(Yte)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a classifier: \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# instantiate classifier object\n",
    "classifier = RandomForestClassifier(n_estimators=50)\n",
    "\n",
    "# fit the classification model on training set\n",
    "classifier.fit(Xtr, Ytr)\n",
    "\n",
    "# make predictions for testing set\n",
    "pred_rf = classifier.predict(Xte) \n",
    "\n",
    "print(\"True Class / Predicted class\")\n",
    "print(np.vstack((Yte[0:10],pred_rf[0:10])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://cs.stanford.edu/people/karpathy/svmjs/demo/demoforest.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Estimation\n",
    "\n",
    "Q: What evaluation metrics are available?\n",
    "\n",
    "<b>Loss Functions</b>\n",
    "\n",
    "- `metrics.zero_one(y_true, y_pred)`\n",
    "Zero-One classification loss\n",
    "- `metrics.hinge_loss(y_true, pred_decision[, ...])`\n",
    "Cumulated hinge loss (non-regularized).\n",
    "- `metrics.mean_square_error(y_true, y_pred)`\n",
    "Mean square error regression loss\n",
    "\n",
    "<b>Score Functions</b>\n",
    "\n",
    "- `metrics.zero_one_loss(y_true, y_pred)`\n",
    "Zero-One classification score\n",
    "- `metrics.auc(x, y)`\n",
    "Compute Area Under the Curve (AUC)\n",
    "- `metrics.precision_score(y_true, y_pred[, ...])`\n",
    "Compute the precision\n",
    "- `metrics.recall_score(y_true, y_pred[, pos_label])`\n",
    "Compute the recall\n",
    "- `metrics.fbeta_score(y_true, y_pred, beta[, ...])`\n",
    "Compute fbeta score\n",
    "- `metrics.f1_score(y_true, y_pred[, pos_label])`\n",
    "Compute f1 score\n",
    "\n",
    "<b>Evaluation Plots</b>\n",
    "- `metrics.confusion_matrix(y_true, y_pred[, ...])` Compute confusion matrix to evaluate the accuracy of a classification\n",
    "- `metrics.roc_curve(y_true, y_score)` Compute Receiver operating characteristic (ROC)\n",
    "- `metrics.precision_recall_curve(y_true, ...)` Compute precision-recall pairs for different probability thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute zero-one loss / score & confusion matrix\n",
    "from sklearn import metrics\n",
    "\n",
    "rf_01 = metrics.zero_one_loss(Yte, pred_rf) # zero-one loss\n",
    "rf_01_score = metrics.accuracy_score(Yte, pred_rf) # zero-one score\n",
    "rf_confmat = metrics.confusion_matrix(Yte, pred_rf) # conf mat\n",
    "\n",
    "print(\"Zero-One Loss: \" + str(rf_01))\n",
    "print(\"Zero-One Score: \" + str(rf_01_score))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(\"[i, j] is the # of objects truly in group i but predicted to be in group j\")\n",
    "print(rf_confmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(rf_confmat, annot=True,  fmt='', \n",
    "            xticklabels=[str(x) for x in range(10)], yticklabels=[str(x) for x in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot some digits that we get wrong\n",
    "wrong = np.where(pred_rf != Yte)[0][:9]\n",
    "\n",
    "f,axs = plt.subplots(3,3,figsize=(7, 7))\n",
    "objarr = np.empty_like(axs)\n",
    "\n",
    "for n, ax in enumerate(axs.flat):\n",
    "    objarr.flat[n] = ax.imshow(np.reshape(Xte[wrong[n]],(8,8)).astype(int),\n",
    "                              cmap='gray_r', interpolation='nearest')\n",
    "    ax.grid(False)\n",
    "    ax.xaxis.set_visible(False)\n",
    "    ax.yaxis.set_visible(False)\n",
    "    ax.set_title(\"True = \" + str(int(Yte[wrong[n]])) +\". Pred = \" + str(int(pred_rf[wrong[n]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute precision and recall\n",
    "# Note: precision & recall are for 2-class; multi-class returns weighted avg. prec/recall\n",
    "\n",
    "rf_precision = metrics.precision_score(Yte, pred_rf,average=\"weighted\") # TP / (TP + FP)\n",
    "rf_recall = metrics.recall_score(Yte, pred_rf,average=\"weighted\") # TP / (TP + FN)\n",
    "\n",
    "print(\"Avg. Precision: \",rf_precision)\n",
    "print(\"Avg. Recall: \", rf_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ROC curve, AUC for RF classifier using digit = 1\n",
    "digit = 1\n",
    "Yte_1 = list(map(lambda x: x == digit and 1. or 0.,Yte)) # does Y = digit\n",
    "\n",
    "pred_rf_prob = classifier.predict_proba(Xte) \n",
    "\n",
    "pred_rf_prob_1 = pred_rf_prob[:,digit]\n",
    "fpr, tpr, thresholds = metrics.roc_curve(Yte_1, pred_rf_prob_1)\n",
    "\n",
    "f, ax = plt.subplots(1,1,figsize=(5, 5))\n",
    "ax.plot(fpr,tpr,'b-',linewidth=3)\n",
    "ax.set_xlim([0.,0.3])\n",
    "ax.set_ylim([0.6,1.0005])\n",
    "ax.set_xlabel(\"False Positive Rate\",size=15)\n",
    "ax.set_ylabel(\"True Positive Rate\",size=15)\n",
    "ax.set_title(\"ROC Curve for NIST digit={} RF Classifier\".format(digit),size=22)\n",
    "print(\"AUC for digit={}: \".format(digit) + str(metrics.auc(fpr,tpr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Tuning the Classifier}$  \n",
    "======================================================\n",
    "\n",
    "For now we have used an RF classifier with the choice of `n_estimators` and the default parameters.\n",
    "\n",
    "Q: How do I choose which model and (hyper) parameters to use?\n",
    "\n",
    " - KNN with what # of neighbors?\n",
    " - SVM which what kernel & bandwidth?\n",
    " - RF with how many estimators and which max_features?\n",
    " - GP with what kernel & bandwidth?\n",
    " \n",
    "**Solution: use `grid_search.GridSearchCV`**:\n",
    "`grid_search.GridSearchCV(estimator, param_grid, loss_func, n_jobs, cv=None)`\n",
    "\n",
    "Computes cv-fold cross-validated loss_func (or score_func) of estimator over a param_grid on n_jobs cores, and returns the best model!\n",
    "\n",
    "Let's see how we can rigorously find the optimal model using cross-validation and grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best Random Forest classifier\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s %(message)s')\n",
    "\n",
    "\n",
    "# explore 3 different forest sizes and 3 choices of mtry\n",
    "parameters = {'n_estimators':[20,50,100],  'max_features':[8,10,'auto'], \n",
    "             'criterion': ['gini','entropy']}\n",
    "rf_tune = model_selection.GridSearchCV(RandomForestClassifier(), parameters, \n",
    "                                   n_jobs = -1, cv = 5,verbose=1)\n",
    "rf_opt = rf_tune.fit(Xtr, Ytr)\n",
    "\n",
    "print(\"Best zero-one score: \" + str(rf_opt.best_score_) + \"\\n\")\n",
    "print(\"Optimal Model:\\n\" + str(rf_opt.best_estimator_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_opt.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Towards interpretability\n",
    "\n",
    "Which features are important in my model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "feature_importances = rf_opt.best_estimator_.feature_importances_\n",
    "feature_importances = feature_importances.reshape(8,8)\n",
    "print(feature_importances.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(feature_importances, cmap=plt.cm.viridis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sklearn FRE**\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html\n",
    "\n",
    "Feature ranking with recursive feature elimination.\n",
    "\n",
    "Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import datasets\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "X = digits['data']\n",
    "Y = digits['target']\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "rfe = RFE(estimator=clf, n_features_to_select=1, step=1)\n",
    "rfe.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "\n",
    "ranking = rfe.ranking_.reshape(digits.images[0].shape)\n",
    "sns.heatmap(ranking, cmap=plt.cm.viridis_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** LightGBM **\n",
    "\n",
    "A fast [gradient boosting framework](http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/) that uses tree-based learning algorithm, generally useful for \"big\" datasets (say 10k rows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on a mac: brew install libomp\n",
    "# !conda install -c conda-forge lightgbm graphviz -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "try:\n",
    "    # To enable interactive mode you should install ipywidgets\n",
    "    # https://github.com/jupyter-widgets/ipywidgets\n",
    "    from ipywidgets import interact, SelectMultiple\n",
    "    INTERACTIVE = True\n",
    "except ImportError:\n",
    "    INTERACTIVE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/microsoft/LightGBM/blob/master/examples/regression/regression.train?raw=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/microsoft/LightGBM/blob/master/examples/regression/regression.test?raw=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('regression.train?raw=true', header=None, sep='\\t')\n",
    "df_test = pd.read_csv('regression.test?raw=true', header=None, sep='\\t')\n",
    "\n",
    "y_train = df_train[0]\n",
    "y_test = df_test[0]\n",
    "X_train = df_train.drop(0, axis=1)\n",
    "X_test = df_test.drop(0, axis=1)\n",
    "print(len(y_train), X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Dataset object for LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "lgb_test = lgb.Dataset(X_test, y_test, reference=lgb_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'num_leaves': 5,\n",
    "    'metric': ['l1', 'l2'],\n",
    "    'verbose': -1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals_result = {}  # to record eval results for plotting\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=100,\n",
    "                valid_sets=[lgb_train, lgb_test],\n",
    "                feature_name=['f' + str(i + 1) for i in range(X_train.shape[-1])],\n",
    "                categorical_feature=[21],\n",
    "                evals_result=evals_result,\n",
    "                verbose_eval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_metric(metric_name):\n",
    "    ax = lgb.plot_metric(evals_result, metric=metric_name, figsize=(10, 5))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if INTERACTIVE:\n",
    "    # create widget to switch between metrics\n",
    "    interact(render_metric, metric_name=params['metric'])\n",
    "else:\n",
    "    render_metric(params['metric'][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot feature importances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_plot_importance(importance_type, max_features=10,\n",
    "                           ignore_zero=True, precision=4):\n",
    "    ax = lgb.plot_importance(gbm, importance_type=importance_type,\n",
    "                             max_num_features=max_features,\n",
    "                             ignore_zero=ignore_zero, figsize=(12, 8),\n",
    "                             precision=precision)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if INTERACTIVE:\n",
    "    # create widget for interactive feature importance plot\n",
    "    interact(render_plot_importance,\n",
    "             importance_type=['split', 'gain'],\n",
    "             max_features=(1, X_train.shape[-1]),\n",
    "             precision=(0, 10))\n",
    "else:\n",
    "    render_plot_importance(importance_type='split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_tree(tree_index, show_info, precision=4):\n",
    "    show_info = None if 'None' in show_info else show_info\n",
    "    return lgb.create_tree_digraph(gbm, tree_index=tree_index,\n",
    "                                   show_info=show_info, precision=precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install graphviz pydot -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if INTERACTIVE:\n",
    "    # create widget to switch between trees and control info in nodes\n",
    "    interact(render_tree,\n",
    "             tree_index=(0, gbm.num_trees() - 1),\n",
    "             show_info=SelectMultiple(  # allow multiple values to be selected\n",
    "                 options=['None',\n",
    "                          'split_gain',\n",
    "                          'internal_value',\n",
    "                          'internal_count',\n",
    "                          'leaf_count'],\n",
    "                 value=['None']),\n",
    "             precision=(0, 10))\n",
    "    tree = None\n",
    "else:\n",
    "    tree = render_tree(84, ['None'])\n",
    "tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
